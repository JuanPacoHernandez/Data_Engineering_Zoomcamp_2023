{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c98c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b401ba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/09 14:19:25 WARN Utils: Your hostname, M710-ASUS resolves to a loopback address: 127.0.1.1; using 192.168.1.72 instead (on interface wlp1s0)\n",
      "23/03/09 14:19:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/09 14:19:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Q1 Creating Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5d9ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    }
   ],
   "source": [
    "## Output from spark.version command\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8564d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 Average size of Parquet Files\n",
    "\n",
    "## Run wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\n",
    "## in bash terminal\n",
    "\n",
    "## Specifying the schema\n",
    "schema = types.StructType([\n",
    "    types.StructField('hvfhvs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PUlocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOlocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])\n",
    "\n",
    "## Reading csv.gz file\n",
    "df = spark.read.option(\"header\", \"true\").schema(schema).csv(\"data/fhvhv_tripdata_2021-06.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff3159f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 0) / 1]\r",
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/09 14:20:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: dispatching_base_num, pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, SR_Flag, Affiliated_base_number\n",
      " Schema: hvfhvs_license_num, dispatching_base_num, pickup_datetime, dropoff_datetime, PUlocationID, DOlocationID, SR_Flag\n",
      "Expected: hvfhvs_license_num but found: dispatching_base_num\n",
      "CSV file: file:///home/m710/Escritorio/Data_Engineering_Zoomcamp/homeworks/homeworkWeek5/data/fhvhv_tripdata_2021-06.csv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Partition by 12\n",
    "df = df.repartition(12)\n",
    "\n",
    "## Write as parquet format\n",
    "df.write.parquet('data/fhvhv/2021/06', mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "967bb25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "450872"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3 How many taxi trips were there on June 15?\n",
    "## Reading the parquet file\n",
    "df = spark.read.parquet('data/fhvhv/2021/06/')\n",
    "df.withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .filter(\"pickup_date = '2021-06-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a424b659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|pickup_date|duration|\n",
      "+-----------+--------+\n",
      "| 2021-06-22|    null|\n",
      "| 2021-06-04|    null|\n",
      "| 2021-06-20|    null|\n",
      "| 2021-06-27|    null|\n",
      "| 2021-06-28|    null|\n",
      "| 2021-06-01|    null|\n",
      "| 2021-06-17|    null|\n",
      "| 2021-06-13|    null|\n",
      "| 2021-06-19|    null|\n",
      "| 2021-07-01|    null|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q4 How long was the longest trip in Hours?\n",
    "\n",
    "## Creating the SQL table\n",
    "df.createOrReplaceTempView('fhvhv_2021_06')\n",
    "\n",
    "## Querying the SQL table\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    to_date(pickup_datetime) AS pickup_date,\n",
    "    MAX((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 60) AS duration\n",
    "FROM \n",
    "    fhvhv_2021_06\n",
    "GROUP BY pickup_date\n",
    "ORDER BY duration DESC\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dffa3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 Sparkâ€™s User Interface which shows application's dashboard runs on which local port?\n",
    "\n",
    "Port= 4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d203fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 Using the zone lookup data and the fhvhv June 2021 data, \n",
    "# what is the name of the most frequent pickup location zone?\n",
    "\n",
    "## Reading csv files\n",
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField('LocationID', types.IntegerType(), True),\n",
    "    types.StructField('Borough', types.StringType(), True),\n",
    "    types.StructField('Zones', types.StringType(), True),\n",
    "    types.StructField('service_zone', types.StringType(), True)\n",
    "])\n",
    "df_zones = spark.read.option(\"header\", \"true\").schema(schema).csv(\"data/taxi_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efbaf0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating SQL table\n",
    "df_zones.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4da1744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/09 14:43:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: LocationID, Zone\n",
      " Schema: LocationID, Zones\n",
      "Expected: Zones but found: Zone\n",
      "CSV file: file:///home/m710/Escritorio/Data_Engineering_Zoomcamp/homeworks/homeworkWeek5/data/taxi_zone_lookup.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|               Zone| Count|\n",
      "+-------------------+------+\n",
      "|                 NA|548735|\n",
      "|Crown Heights North|236244|\n",
      "|        JFK Airport|224571|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Querying\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    zones.Zones as Zone,\n",
    "    COUNT(1) as Count\n",
    "FROM \n",
    "    fhvhv_2021_06 fhv LEFT JOIN zones ON fhv.PULocationID = zones.LocationID\n",
    "GROUP BY Zone\n",
    "ORDER BY Count DESC\n",
    "LIMIT 3;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad07c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dccdc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
